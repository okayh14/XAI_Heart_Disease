{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda444df-a7d2-4dd0-8880-0cf43b86533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 1 – Import libraries (Classification notebook)\n",
    "# --------------------------------------------\n",
    "# What this cell does:\n",
    "# - Imports all Python libraries we need for:\n",
    "#   * Data handling (NumPy, pandas)\n",
    "#   * Splitting data into train / validation / test\n",
    "#   * Preprocessing (scaling numeric features, encoding categorical features)\n",
    "#   * Building pipelines (preprocess + model together)\n",
    "#   * Evaluating classification performance\n",
    "#   * Training 4 models on the SAME classification dataset:\n",
    "#       - LinearSVC\n",
    "#       - Logistic Regression (correct for classification)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np  # NumPy: fast numerical operations and array handling\n",
    "import pandas as pd  # pandas: work with tabular data (dataframes similar to Excel tables)\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # function to split data into train / validation / test sets\n",
    "\n",
    "from sklearn.compose import ColumnTransformer  # lets us apply different preprocessing to different columns\n",
    "from sklearn.preprocessing import OneHotEncoder  # transforms categorical (text) columns into numeric dummy variables\n",
    "from sklearn.preprocessing import StandardScaler  # scales numeric features (mean 0, std 1), useful for KNN and logistic regression\n",
    "\n",
    "from sklearn.pipeline import Pipeline  # allows us to chain preprocessing and model into one pipeline object\n",
    "\n",
    "# ---- Classification metrics ----\n",
    "from sklearn.metrics import accuracy_score  # basic classification metric: fraction of correct predictions\n",
    "from sklearn.metrics import classification_report  # detailed classification metrics (precision, recall, f1-score for each class)\n",
    "\n",
    "# ---- Models we will use on the CLASSIFICATION dataset ----\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model (well-suited for binary classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acf769e-7d63-4ffc-bb95-f3a9c552656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (918, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n",
      "\n",
      "Missing values per column:\n",
      "Age               0\n",
      "Sex               0\n",
      "ChestPainType     0\n",
      "RestingBP         0\n",
      "Cholesterol       0\n",
      "FastingBS         0\n",
      "RestingECG        0\n",
      "MaxHR             0\n",
      "ExerciseAngina    0\n",
      "Oldpeak           0\n",
      "ST_Slope          0\n",
      "HeartDisease      0\n",
      "dtype: int64\n",
      "\n",
      "Target variable selected: HeartDisease\n",
      "Unique values in target: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2 – Load the Heart Disease dataset\n",
    "# --------------------------------------------\n",
    "# What this cell does:\n",
    "# - Loads the dataset from the Kaggle input directory using the correct folder name.\n",
    "# - Displays the first few rows to understand the structure.\n",
    "# - Prints dataset shape, data types, and missing values.\n",
    "# - Confirms that the target column (HeartDisease) exists and is binary.\n",
    "# ============================================\n",
    "\n",
    "# Load the dataset (replace folder name if yours is different)\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "# ^ This file path works when the dataset folder is: heart-failure-prediction\n",
    "\n",
    "# Show the first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Print dataset shape (rows, columns)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Show information about column types and non-null counts\n",
    "df.info()\n",
    "\n",
    "# Print missing values for each column\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Set the target variable for classification\n",
    "target = \"HeartDisease\"  # 1 = disease present, 0 = no disease\n",
    "\n",
    "# Check uniqueness of the target values\n",
    "print(\"\\nTarget variable selected:\", target)\n",
    "print(\"Unique values in target:\", df[target].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98186673-6919-4a51-be28-d28e7caa725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target value counts (0 = no disease, 1 = disease):\n",
      "HeartDisease\n",
      "1    508\n",
      "0    410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric feature columns:\n",
      "['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
      "\n",
      "Categorical feature columns:\n",
      "['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 3 – Separate features & target, detect feature types\n",
    "# --------------------------------------------\n",
    "# What this cell does:\n",
    "# - Splits the DataFrame into:\n",
    "#     * X = input features (all columns except the target)\n",
    "#     * y = target labels (HeartDisease)\n",
    "# - Checks the distribution of the target (how many 0 vs 1).\n",
    "# - Automatically detects:\n",
    "#     * numeric feature columns\n",
    "#     * categorical feature columns\n",
    "# - Prints lists of numeric and categorical columns so we know\n",
    "#   what will be scaled and what will be one-hot encoded later.\n",
    "# ============================================\n",
    "\n",
    "# Separate input features (X) and target labels (y)\n",
    "X = df.drop(columns=[target])  # X = all columns except HeartDisease\n",
    "y = df[target]                 # y = the HeartDisease column only\n",
    "\n",
    "# Show basic information about the target distribution (class balance)\n",
    "print(\"Target value counts (0 = no disease, 1 = disease):\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Automatically detect numeric feature columns (int or float)\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "# Automatically detect categorical feature columns (non-numeric types)\n",
    "categorical_features = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric feature columns:\")\n",
    "print(numeric_features)\n",
    "\n",
    "print(\"\\nCategorical feature columns:\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2c4a81-2ea2-480d-9ef5-4a76c07a73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:      642 rows\n",
      "Validation set size: 138 rows\n",
      "Test set size:       138 rows\n",
      "\n",
      "Percentage breakdown:\n",
      "Train:       69.93 %\n",
      "Validation:  15.03 %\n",
      "Test:        15.03 %\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4 – Train / Validation / Test split (70% / 15% / 15%)\n",
    "# --------------------------------------------\n",
    "# What this cell does:\n",
    "# - Splits our dataset into:\n",
    "#       70% Training\n",
    "#       15% Validation\n",
    "#       15% Test\n",
    "# - We first split out 15% for the TEST set.\n",
    "# - Then from the remaining 85%, we calculate how much to split\n",
    "#   to produce exactly 15% VALIDATION overall.\n",
    "# - We use stratify=y to keep the same class balance (0/1)\n",
    "#   in all splits.\n",
    "# ============================================\n",
    "\n",
    "# --- Step 1: Split OFF the Test Set (15%) ---\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.15,      # 15% for final test\n",
    "    random_state=42,\n",
    "    stratify=y           # keep 0/1 ratio consistent\n",
    ")\n",
    "# --- Step 2: From the remaining 85%, split Train and Validation ---\n",
    "# Validation should be 15% of TOTAL.\n",
    "# So inside the remaining 85%, validation proportion is:\n",
    "#       0.15 / 0.85 = ~0.17647\n",
    "validation_ratio = 0.15 / 0.85  # ~0.17647\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=validation_ratio,  # produces 15% overall validation\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Print out the sizes to confirm 70/15/15 split\n",
    "print(\"Train set size:     \", X_train.shape[0], \"rows\")\n",
    "print(\"Validation set size:\", X_val.shape[0], \"rows\")\n",
    "print(\"Test set size:      \", X_test.shape[0], \"rows\")\n",
    "\n",
    "# Optional check: print percentages\n",
    "total = len(df)\n",
    "print(\"\\nPercentage breakdown:\")\n",
    "print(\"Train:      \", round(X_train.shape[0] / total * 100, 2), \"%\")\n",
    "print(\"Validation: \", round(X_val.shape[0] / total * 100, 2), \"%\")\n",
    "print(\"Test:       \", round(X_test.shape[0] / total * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26381091-e9e4-4628-ab34-290836e96dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Preprocessing...\n",
      "Preprocessing abgeschlossen!\n",
      "Neue Anzahl Features: 20\n",
      "\n",
      "So sehen die Daten jetzt aus (erste 3 Zeilen X_train_processed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.178734</td>\n",
       "      <td>0.377473</td>\n",
       "      <td>0.497810</td>\n",
       "      <td>-0.525682</td>\n",
       "      <td>1.313630</td>\n",
       "      <td>-0.843569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>-0.074608</td>\n",
       "      <td>-0.515599</td>\n",
       "      <td>0.709980</td>\n",
       "      <td>-0.525682</td>\n",
       "      <td>-1.568765</td>\n",
       "      <td>1.027790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.074608</td>\n",
       "      <td>-0.673200</td>\n",
       "      <td>0.636182</td>\n",
       "      <td>-0.525682</td>\n",
       "      <td>-0.223647</td>\n",
       "      <td>-0.843569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  RestingBP  Cholesterol  FastingBS     MaxHR   Oldpeak  Sex_F  \\\n",
       "43  -0.178734   0.377473     0.497810  -0.525682  1.313630 -0.843569    0.0   \n",
       "760 -0.074608  -0.515599     0.709980  -0.525682 -1.568765  1.027790    0.0   \n",
       "255 -0.074608  -0.673200     0.636182  -0.525682 -0.223647 -0.843569    1.0   \n",
       "\n",
       "     Sex_M  ChestPainType_ASY  ChestPainType_ATA  ChestPainType_NAP  \\\n",
       "43     1.0                0.0                0.0                1.0   \n",
       "760    1.0                1.0                0.0                0.0   \n",
       "255    0.0                0.0                0.0                1.0   \n",
       "\n",
       "     ChestPainType_TA  RestingECG_LVH  RestingECG_Normal  RestingECG_ST  \\\n",
       "43                0.0             0.0                0.0            1.0   \n",
       "760               0.0             0.0                1.0            0.0   \n",
       "255               0.0             0.0                1.0            0.0   \n",
       "\n",
       "     ExerciseAngina_N  ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  \\\n",
       "43                1.0               0.0            0.0            0.0   \n",
       "760               0.0               1.0            0.0            1.0   \n",
       "255               1.0               0.0            0.0            0.0   \n",
       "\n",
       "     ST_Slope_Up  \n",
       "43           1.0  \n",
       "760          0.0  \n",
       "255          1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5 – Manual Preprocessing (Ohne Pipeline)\n",
    "# --------------------------------------------\n",
    "# Was wir hier tun:\n",
    "# 1. Wir definieren den ColumnTransformer wie vorher.\n",
    "# 2. Wir wenden ihn manuell auf X_train und X_val an.\n",
    "# 3. Wir machen daraus wieder schöne DataFrames mit Spaltennamen.\n",
    "# ============================================\n",
    "\n",
    "# 1. Definition der Transformer (wie vorher)\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False  # Wichtig: False, damit wir eine lesbare Tabelle (Matrix) bekommen\n",
    ")\n",
    "\n",
    "# Der Preprocessor (fasst beides zusammen)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False # Sorgt für schönere Namen ohne \"num__\" oder \"cat__\" Präfix\n",
    ")\n",
    "\n",
    "# 2. Fitten und Transformieren\n",
    "# ACHTUNG: fit() darf NUR auf X_train passieren!\n",
    "print(\"Starte Preprocessing...\")\n",
    "\n",
    "# Lernen (fit) und Anwenden (transform) auf Trainingsdaten\n",
    "X_train_np = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Nur Anwenden (transform) auf Validierungsdaten (nichts neues lernen!)\n",
    "X_val_np = preprocessor.transform(X_val)\n",
    "\n",
    "# Nur Anwenden (transform) auf Testdaten\n",
    "X_test_np = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# 3. Feature Namen retten (Wichtig für XAI!)\n",
    "# Wir holen uns die Namen der neuen Spalten direkt aus dem Preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Wir wandeln die NumPy Arrays zurück in Pandas DataFrames\n",
    "# Das macht es viel leichter zu verstehen, was passiert ist\n",
    "X_train_processed = pd.DataFrame(X_train_np, columns=feature_names, index=X_train.index)\n",
    "X_val_processed = pd.DataFrame(X_val_np, columns=feature_names, index=X_val.index)\n",
    "X_test_processed = pd.DataFrame(X_test_np, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"Preprocessing abgeschlossen!\")\n",
    "print(f\"Neue Anzahl Features: {X_train_processed.shape[1]}\")\n",
    "print(\"\\nSo sehen die Daten jetzt aus (erste 3 Zeilen X_train_processed):\")\n",
    "display(X_train_processed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49033f75-247b-4237-93e0-9f47107f428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest – Validation accuracy: 0.8406\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82        62\n",
      "           1       0.85      0.87      0.86        76\n",
      "\n",
      "    accuracy                           0.84       138\n",
      "   macro avg       0.84      0.84      0.84       138\n",
      "weighted avg       0.84      0.84      0.84       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6 – Train & Evaluate Random Forest\n",
    "# --------------------------------------------\n",
    "# Wir nutzen nun den RandomForestClassifier, der für \n",
    "# XAI-Methoden wie SHAP oder LIME besonders spannend ist.\n",
    "# ============================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Modell definieren\n",
    "# random_state=42 sorgt dafür, dass deine Ergebnisse reproduzierbar bleiben.\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=None,   # Die Bäume wachsen, bis alle Blätter rein sind\n",
    "    random_state=39,\n",
    "    n_jobs=-1         # Nutzt alle verfügbaren CPU-Kerne für schnelleres Training\n",
    ")\n",
    "\n",
    "# 2. Modell trainieren\n",
    "# Wir nutzen weiterhin die vorverarbeiteten Daten (X_train_processed)\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# 3. Vorhersagen machen\n",
    "y_val_pred = rf_model.predict(X_val_processed)\n",
    "\n",
    "# 4. Evaluieren\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Random Forest – Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d4dd3a8-b97e-412f-9c16-5a0910489b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINALE PRÜFUNG (Test Set) ---\n",
      "Test Set Accuracy: 0.8841\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        62\n",
      "           1       0.89      0.89      0.89        76\n",
      "\n",
      "    accuracy                           0.88       138\n",
      "   macro avg       0.88      0.88      0.88       138\n",
      "weighted avg       0.88      0.88      0.88       138\n",
      "\n",
      "------------------------------\n",
      "Vergleich Accuracy: Val=0.8406 vs. Test=0.8841\n",
      "-> Das Modell ist stabil (kein großes Overfitting).\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7 – Final Evaluation on Test Set\n",
    "# --------------------------------------------\n",
    "# ACHTUNG: Das machen wir nur EINMAL ganz am Schluss.\n",
    "# Wenn dieses Ergebnis schlecht ist, müssen wir eigentlich\n",
    "# ganz von vorne anfangen (neue Features, anderes Modell),\n",
    "# aber wir dürfen nicht einfach an kleinen Schräubchen drehen,\n",
    "# bis dieses Ergebnis passt.\n",
    "# ============================================\n",
    "\n",
    "print(\"--- FINALE PRÜFUNG (Test Set) ---\")\n",
    "\n",
    "# 1. Vorhersagen auf den bereits verarbeiteten Testdaten machen\n",
    "# (X_test_processed haben wir in Cell 5 schon erstellt)\n",
    "y_test_pred = rf_model.predict(X_test_processed)\n",
    "\n",
    "# 2. Genauigkeit berechnen\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Vergleich: War Validierung und Test ähnlich?\n",
    "print(\"-\" * 30)\n",
    "print(f\"Vergleich Accuracy: Val={val_accuracy:.4f} vs. Test={test_accuracy:.4f}\")\n",
    "if abs(val_accuracy - test_accuracy) < 0.05:\n",
    "    print(\"-> Das Modell ist stabil (kein großes Overfitting).\")\n",
    "else:\n",
    "    print(\"-> Vorsicht: Großer Unterschied zwischen Validierung und Test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "297dab81-e933-41a1-92ee-2898849e8569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v_01_Random_Forest_Model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rf_model, 'v_01_Random_Forest_Model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af494fc6-9117-49f1-a4f8-5cfcae4338d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
